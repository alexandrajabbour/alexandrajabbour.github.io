<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Alexandra Jabbour</title>
<meta name="description" content="Alexandra Jabbour's Academic Site
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-196485093-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-196485093-1');
  </script>



    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:alexandra.jabbour@umontreal.ca"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.fr/citations?hl=fr&user=V34hXB8AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/alexandrajabbour" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://twitter.com/alexjabbour" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          
          <!-- projects -->
          <li class="nav-item ">
            <a class="nav-link" href="/projects/">
              projects
              
            </a>
          </li>
          
          
          <!-- Other pages -->
          

          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Alexandra</span>   Jabbour 
    </h1>
     <p class="desc">Ph.D. candidate in Political Science</a></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
     <p align="justify">I am a PhD candidate in Political Science at the University of Montreal (Canada). During my PhD, I have been a Visiting Researcher at the Department of Political Science at Aarhus University. I am funded by the Fond de recherche du Québec Société et Culture (FRQSC).</p>

<p align="justify">Starting in the fall of 2023, I will be a Max Weber Fellow at the European University Institute.</p>

<p align="justify">I do research at the intersection of political opinion, geography and economy. I am also interested in questions on political behavior, group identity, and the political consequences of housing market. Methodologically, I have a keen interest for causal inference, experimental methods in social sciences and smart identification strategies.</p>

<p align="justify">I am currently a research fellow at the Canada Research Chair in Electoral Democracy led by Ruth Dassonneville and affiliated with the Center for the Study of Democratic Citizenship. I am also a catalyst of the Berkeley Initiative for Transparency in the Social Sciences.</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">

      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected Publications:</h2>
  <p>* denotes equal contribution</p>
  <ol class="bibliography"><li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="norouzi-etal-2021-code" class="col-sm-8">
    
      <div class="title">Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data</div>
      <div class="author">
        

        

           
              
              
                
                  Sajad Norouzi,
                
              
                  
          
        

        

           
              
              
                
                  Keyi Tang,
                
              
                  
          
        

        

           
              
                  
                and <em>Yanshuai Cao</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
    
      <a href="https://aclanthology.org/2021.acl-short.98/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/BorealisAI/code-gen-TAE" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{norouzi-etal-2021-code,
  abbr = {ACL},
  title = {Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data},
  author = {Norouzi, Sajad and Tang, Keyi and Cao, Yanshuai},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  month = aug,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.acl-short.98},
  doi = {10.18653/v1/2021.acl-short.98},
  pages = {776--785},
  code = {https://github.com/BorealisAI/code-gen-TAE},
  selected = {true},
  html = {https://aclanthology.org/2021.acl-short.98/}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#norouzi-etal-2021-code-bibtex" aria-expanded="false" aria-controls="norouzi-etal-2021-code-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="norouzi-etal-2021-code-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{norouzi-etal-2021-code,
  abbr = {ACL},
  title = {Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data},
  author = {Norouzi, Sajad and Tang, Keyi and Cao, Yanshuai},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  month = aug,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.acl-short.98},
  doi = {10.18653/v1/2021.acl-short.98},
  pages = {776--785},
  code = {https://github.com/BorealisAI/code-gen-TAE},
  selected = {true},
  html = {https://aclanthology.org/2021.acl-short.98/}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#norouzi-etal-2021-code-abstract" aria-expanded="false" aria-controls="norouzi-etal-2021-code-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="norouzi-etal-2021-code-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_norouzi-etal-2021-code-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="xu-etal-2021-optimizing" class="col-sm-8">
    
      <div class="title">Optimizing Deeper Transformers on Small Datasets</div>
      <div class="author">
        

        

           
              
              
                
                  Peng Xu,
                
              
                  
          
        

        

           
              
              
                
                  Dhruv Kumar,
                
              
                  
          
        

        

           
              
              
                
                  Wei Yang,
                
              
                  
          
        

        

           
              
              
                
                  Wenjie Zi,
                
              
                  
          
        

        

           
              
              
                
                  Keyi Tang,
                
              
                  
          
        

        

           
              
              
                
                  Chenyang Huang,
                
              
                  
          
        

        

           
              
              
                
                  Jackie Chi Kit Cheung,
                
              
                  
          
        

        

           
              
              
                
                  Simon J.D. Prince,
                
              
                  
          
        

        

           
              
                  
                and <em>Yanshuai Cao</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
    
      <a href="https://aclanthology.org/2021.acl-long.163" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/BorealisAI/DT-Fixup" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{xu-etal-2021-optimizing,
  abbr = {ACL},
  title = {Optimizing Deeper Transformers on Small Datasets},
  author = {Xu, Peng and Kumar, Dhruv and Yang, Wei and Zi, Wenjie and Tang, Keyi and Huang, Chenyang and Cheung, Jackie Chi Kit and Prince, Simon J.D. and Cao, Yanshuai},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  month = aug,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.acl-long.163},
  doi = {10.18653/v1/2021.acl-long.163},
  pages = {2089--2102},
  html = {https://aclanthology.org/2021.acl-long.163},
  code = {https://github.com/BorealisAI/DT-Fixup},
  selected = {true}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#xu-etal-2021-optimizing-bibtex" aria-expanded="false" aria-controls="xu-etal-2021-optimizing-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="xu-etal-2021-optimizing-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{xu-etal-2021-optimizing,
  abbr = {ACL},
  title = {Optimizing Deeper Transformers on Small Datasets},
  author = {Xu, Peng and Kumar, Dhruv and Yang, Wei and Zi, Wenjie and Tang, Keyi and Huang, Chenyang and Cheung, Jackie Chi Kit and Prince, Simon J.D. and Cao, Yanshuai},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  month = aug,
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2021.acl-long.163},
  doi = {10.18653/v1/2021.acl-long.163},
  pages = {2089--2102},
  html = {https://aclanthology.org/2021.acl-long.163},
  code = {https://github.com/BorealisAI/DT-Fixup},
  selected = {true}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#xu-etal-2021-optimizing-abstract" aria-expanded="false" aria-controls="xu-etal-2021-optimizing-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="xu-etal-2021-optimizing-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_xu-etal-2021-optimizing-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AISTATS</abbr>
    
  
  </div>

  <div id="pmlr-v108-cao20a" class="col-sm-8">
    
      <div class="title">Better Long-Range Dependency By Bootstrapping A Mutual Information Regularizer</div>
      <div class="author">
        

        

           
              
                     
                <em>Yanshuai Cao*</em>,
              
                  
          
        

        

           
              
                  
                
                  and Peng Xu*
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
      <a href="http://arxiv.org/abs/1905.11978" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="http://proceedings.mlr.press/v108/cao20a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="http://proceedings.mlr.press/v108/cao20a/cao20a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/BorealisAI/BMI" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this work, we develop a novel regularizer to improve the learning of long-range dependency of sequence data. Applied on language modelling, our regularizer expresses the inductive bias that sequence variables should have high mutual information even though the model might not see abundant observations for complex long-range dependency. We show how the “next sentence prediction (classification)" heuristic can be derived in a principled way from our mutual information estimation framework, and be further extended to maximize the mutual information of sequence variables. The proposed approach not only is effective at increasing the mutual information of segments under the learned model but more importantly, leads to a higher likelihood on holdout data, and improved generation quality. Code is releasedat https://github.com/BorealisAI/BMI.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{pmlr-v108-cao20a,
  abbr = {AISTATS},
  title = {Better Long-Range Dependency By Bootstrapping A Mutual Information Regularizer},
  author = {Cao, Yanshuai and Xu, Peng},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = {3991--4001},
  year = {2020},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  volume = {108},
  series = {Proceedings of Machine Learning Research},
  address = {Online},
  month = {26--28 Aug},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v108/cao20a/cao20a.pdf},
  html = {http://proceedings.mlr.press/v108/cao20a.html},
  arxiv = {1905.11978},
  code = {https://github.com/BorealisAI/BMI},
  selected = {true},
  equal_last = {Cao, Xu}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v108-cao20a-bibtex" aria-expanded="false" aria-controls="pmlr-v108-cao20a-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v108-cao20a-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{pmlr-v108-cao20a,
  abbr = {AISTATS},
  title = {Better Long-Range Dependency By Bootstrapping A Mutual Information Regularizer},
  author = {Cao, Yanshuai and Xu, Peng},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = {3991--4001},
  year = {2020},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  volume = {108},
  series = {Proceedings of Machine Learning Research},
  address = {Online},
  month = {26--28 Aug},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v108/cao20a/cao20a.pdf},
  html = {http://proceedings.mlr.press/v108/cao20a.html},
  arxiv = {1905.11978},
  code = {https://github.com/BorealisAI/BMI},
  selected = {true},
  equal_last = {Cao, Xu}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v108-cao20a-abstract" aria-expanded="false" aria-controls="pmlr-v108-cao20a-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v108-cao20a-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>In this work, we develop a novel regularizer to improve the learning of long-range dependency of sequence data. Applied on language modelling, our regularizer expresses the inductive bias that sequence variables should have high mutual information even though the model might not see abundant observations for complex long-range dependency. We show how the “next sentence prediction (classification)" heuristic can be derived in a principled way from our mutual information estimation framework, and be further extended to maximize the mutual information of sequence variables. The proposed approach not only is effective at increasing the mutual information of segments under the learned model but more importantly, leads to a higher likelihood on holdout data, and improved generation quality. Code is releasedat https://github.com/BorealisAI/BMI.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_pmlr-v108-cao20a-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="pmlr-v119-huang20c" class="col-sm-8">
    
      <div class="title">Evaluating Lossy Compression Rates of Deep Generative Models</div>
      <div class="author">
        

        

           
              
              
                
                  Sicong Huang*,
                
              
                  
          
        

        

           
              
              
                
                  Alireza Makhzani*,
                
              
                  
          
        

        

           
              
                     
                <em>Yanshuai Cao</em>,
              
                  
          
        

        

           
              
                  
                
                  and Roger Grosse
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 37th International Conference on Machine Learning</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
      <a href="http://arxiv.org/abs/2008.06653" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="http://proceedings.mlr.press/v119/huang20c.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="http://proceedings.mlr.press/v119/huang20c/huang20c.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/huangsicong/rate_distortion" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The field of deep generative modeling has succeeded in producing astonishingly realistic-seeming images and audio, but quantitative evaluation remains a challenge. Log-likelihood is an appealing metric due to its grounding in statistics and information theory, but it can be challenging to estimate for implicit generative models, and scalar-valued metrics give an incomplete picture of a model’s quality. In this work, we propose to use rate distortion (RD) curves to evaluate and compare deep generative models. While estimating RD curves is seemingly even more computationally demanding than log-likelihood estimation, we show that we can approximate the entire RD curve using nearly the same computations as were previously used to achieve a single log-likelihood estimate. We evaluate lossy compression rates of VAEs, GANs, and adversarial autoencoders (AAEs) on the MNIST and CIFAR10 datasets. Measuring the entire RD curve gives a more complete picture than scalar-valued metrics, and we arrive at a number of insights not obtainable from log-likelihoods alone.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{pmlr-v119-huang20c,
  abbr = {ICML},
  title = {Evaluating Lossy Compression Rates of Deep Generative Models},
  author = {Huang, Sicong and Makhzani, Alireza and Cao, Yanshuai and Grosse, Roger},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {4444--4454},
  year = {2020},
  editor = {III, Hal Daumé and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  address = {Virtual},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v119/huang20c/huang20c.pdf},
  html = {http://proceedings.mlr.press/v119/huang20c.html},
  arxiv = {2008.06653},
  code = {https://github.com/huangsicong/rate_distortion},
  selected = {true},
  equal_last = {Huang, Makhzani}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v119-huang20c-bibtex" aria-expanded="false" aria-controls="pmlr-v119-huang20c-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v119-huang20c-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{pmlr-v119-huang20c,
  abbr = {ICML},
  title = {Evaluating Lossy Compression Rates of Deep Generative Models},
  author = {Huang, Sicong and Makhzani, Alireza and Cao, Yanshuai and Grosse, Roger},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {4444--4454},
  year = {2020},
  editor = {III, Hal Daumé and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  address = {Virtual},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v119/huang20c/huang20c.pdf},
  html = {http://proceedings.mlr.press/v119/huang20c.html},
  arxiv = {2008.06653},
  code = {https://github.com/huangsicong/rate_distortion},
  selected = {true},
  equal_last = {Huang, Makhzani}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v119-huang20c-abstract" aria-expanded="false" aria-controls="pmlr-v119-huang20c-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v119-huang20c-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>The field of deep generative modeling has succeeded in producing astonishingly realistic-seeming images and audio, but quantitative evaluation remains a challenge. Log-likelihood is an appealing metric due to its grounding in statistics and information theory, but it can be challenging to estimate for implicit generative models, and scalar-valued metrics give an incomplete picture of a model’s quality. In this work, we propose to use rate distortion (RD) curves to evaluate and compare deep generative models. While estimating RD curves is seemingly even more computationally demanding than log-likelihood estimation, we show that we can approximate the entire RD curve using nearly the same computations as were previously used to achieve a single log-likelihood estimate. We evaluate lossy compression rates of VAEs, GANs, and adversarial autoencoders (AAEs) on the MNIST and CIFAR10 datasets. Measuring the entire RD curve gives a more complete picture than scalar-valued metrics, and we arrive at a number of insights not obtainable from log-likelihoods alone.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_pmlr-v119-huang20c-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="pmlr-v119-xu20a" class="col-sm-8">
    
      <div class="title">On Variational Learning of Controllable Representations for Text without Supervision</div>
      <div class="author">
        

        

           
              
              
                
                  Peng Xu,
                
              
                  
          
        

        

           
              
              
                
                  Jackie Chi Kit Cheung,
                
              
                  
          
        

        

           
              
                  
                and <em>Yanshuai Cao</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 37th International Conference on Machine Learning</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
      <a href="http://arxiv.org/abs/1905.11975" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="http://proceedings.mlr.press/v119/xu20a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="http://proceedings.mlr.press/v119/xu20a/xu20a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/BorealisAI/CP-VAE" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolating or extrapolating in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer, and is capable of performing more flexible fine-grained control over text generation than existing methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{pmlr-v119-xu20a,
  abbr = {ICML},
  title = {On Variational Learning of Controllable Representations for Text without Supervision},
  author = {Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {10534--10543},
  year = {2020},
  editor = {III, Hal Daumé and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  address = {Virtual},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v119/xu20a/xu20a.pdf},
  url = {http://proceedings.mlr.press/v119/xu20a.html},
  html = {http://proceedings.mlr.press/v119/xu20a.html},
  arxiv = {1905.11975},
  code = {https://github.com/BorealisAI/CP-VAE},
  selected = {true}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v119-xu20a-bibtex" aria-expanded="false" aria-controls="pmlr-v119-xu20a-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v119-xu20a-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{pmlr-v119-xu20a,
  abbr = {ICML},
  title = {On Variational Learning of Controllable Representations for Text without Supervision},
  author = {Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {10534--10543},
  year = {2020},
  editor = {III, Hal Daumé and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  address = {Virtual},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v119/xu20a/xu20a.pdf},
  url = {http://proceedings.mlr.press/v119/xu20a.html},
  html = {http://proceedings.mlr.press/v119/xu20a.html},
  arxiv = {1905.11975},
  code = {https://github.com/BorealisAI/CP-VAE},
  selected = {true}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#pmlr-v119-xu20a-abstract" aria-expanded="false" aria-controls="pmlr-v119-xu20a-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="pmlr-v119-xu20a-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolating or extrapolating in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer, and is capable of performing more flexible fine-grained control over text generation than existing methods.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_pmlr-v119-xu20a-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="Cao2018Improving" class="col-sm-8">
    
      <div class="title">Improving GAN Training via Binarized Representation Entropy (BRE) Regularization</div>
      <div class="author">
        

        

           
              
                     
                <em>Yanshuai Cao</em>,
              
                  
          
        

        

           
              
              
                
                  Gavin Weiguang Ding,
                
              
                  
          
        

        

           
              
              
                
                  Kry Yik-Chau Lui,
                
              
                  
          
        

        

           
              
                  
                
                  and Ruitong Huang
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations, ICLR</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
      <a href="http://arxiv.org/abs/1805.03644" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://openreview.net/forum?id=BkLhaGZRW" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/BorealisAI/bre-gan" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @article{Cao2018Improving,
  abbr = {ICLR},
  title = {Improving GAN Training via Binarized Representation Entropy (BRE) Regularization},
  author = {Cao, Yanshuai and Ding, Gavin Weiguang and Lui, Kry Yik-Chau and Huang, Ruitong},
  journal = {International Conference on Learning Representations, {ICLR}},
  year = {2018},
  html = {https://openreview.net/forum?id=BkLhaGZRW},
  arxiv = {1805.03644},
  code = {https://github.com/BorealisAI/bre-gan},
  selected = {true}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#Cao2018Improving-bibtex" aria-expanded="false" aria-controls="Cao2018Improving-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="Cao2018Improving-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@article{Cao2018Improving,
  abbr = {ICLR},
  title = {Improving GAN Training via Binarized Representation Entropy (BRE) Regularization},
  author = {Cao, Yanshuai and Ding, Gavin Weiguang and Lui, Kry Yik-Chau and Huang, Ruitong},
  journal = {International Conference on Learning Representations, {ICLR}},
  year = {2018},
  html = {https://openreview.net/forum?id=BkLhaGZRW},
  arxiv = {1805.03644},
  code = {https://github.com/BorealisAI/bre-gan},
  selected = {true}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#Cao2018Improving-abstract" aria-expanded="false" aria-controls="Cao2018Improving-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="Cao2018Improving-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_Cao2018Improving-bibtex" style="outline:none;">BibTeX</button> -->
    




</li>
<li><div class="card">
  <div class="card-body">
    
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="bose-etal-2018-adversarial" class="col-sm-8">
    
      <div class="title">Adversarial Contrastive Estimation</div>
      <div class="author">
        

        

           
              
              
                
                  Avishek Joey Bose*,
                
              
                  
          
        

        

           
              
              
                
                  Huan Ling*,
                
              
                  
          
        

        

           
              
                  
                and <em>Yanshuai Cao*</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
    
      <a href="http://arxiv.org/abs/1805.03642" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://www.aclweb.org/anthology/P18-1094/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
      
      
    
    
    </div>

    
    
    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <div class="card card-body">
    <pre> @inproceedings{bose-etal-2018-adversarial,
  abbr = {ACL},
  title = {Adversarial Contrastive Estimation},
  author = {Bose, Avishek Joey and Ling, Huan and Cao, Yanshuai},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month = jul,
  year = {2018},
  address = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url = {https://www.aclweb.org/anthology/P18-1094},
  doi = {10.18653/v1/P18-1094},
  pages = {1021--1032},
  arxiv = {1805.03642},
  html = {https://www.aclweb.org/anthology/P18-1094/},
  selected = {true},
  equal_last = {Bose, Ling, Cao}
}
 </pre>
    </div>
    </div>

  </div>
</div>

  </div>
</div>


<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#bose-etal-2018-adversarial-bibtex" aria-expanded="false" aria-controls="bose-etal-2018-adversarial-bibtex"> -->
<!--  Bibtex -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="bose-etal-2018-adversarial-bibtex"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>@inproceedings{bose-etal-2018-adversarial,
  abbr = {ACL},
  title = {Adversarial Contrastive Estimation},
  author = {Bose, Avishek Joey and Ling, Huan and Cao, Yanshuai},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month = jul,
  year = {2018},
  address = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url = {https://www.aclweb.org/anthology/P18-1094},
  doi = {10.18653/v1/P18-1094},
  pages = {1021--1032},
  arxiv = {1805.03642},
  html = {https://www.aclweb.org/anthology/P18-1094/},
  selected = {true},
  equal_last = {Bose, Ling, Cao}
}
</pre> -->
<!--   </div> -->
<!-- </div> -->

<!--      -->
<!-- <p> -->
<!--   <button class="btn btn-primary btnPub" type="button" data-toggle="collapse" data-target="#bose-etal-2018-adversarial-abstract" aria-expanded="false" aria-controls="bose-etal-2018-adversarial-abstract"> -->
<!--  Abstract -->
<!--   </button> -->
<!-- </p> -->
<!-- <div class="collapse" id="bose-etal-2018-adversarial-abstract"> -->
<!--   <div class="card card-body"> -->
<!--     <pre>Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.</pre> -->
<!--   </div> -->
<!-- </div> -->
<!--      -->
<!-- <button class="btn btnId btnPub--BibTex" id="b_bose-etal-2018-adversarial-bibtex" style="outline:none;">BibTeX</button> -->
    




</li></ol>
</div>

    

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:alexandra.jabbour@umontreal.ca"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=RTVRTSsAAAAJ&hl" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/alexandrajabbour" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://twitter.com/alexjabbour" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
